{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c45c9c-d2c2-49e2-a88a-d20a5d292a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab8095a8-bf12-4d8e-9f30-b498588d8ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pytorch_lightning.profilers import PyTorchProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732361d1-41e4-4ede-a28f-b2126e72ed38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NietzscheDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, file_path, tokenizer, batch_size=4):\n",
    "        super(NietzscheDataModule, self).__init__()\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.input_ids = self.tokenizer.encode(text, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = torch.utils.data.TensorDataset(self.input_ids)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381cb31a-5e01-423c-9aa9-81b88bf56993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NietzscheTextGenerator(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(NietzscheTextGenerator, self).__init__()\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = torch.stack(batch)\n",
    "        outputs = self.model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        perplexity = torch.exp(loss)\n",
    "        self.log('perplexity', perplexity, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fef4ab-f1b1-41bf-8ead-1860acc5bf84",
   "metadata": {
    "tags": []
   },
   "source": [
    "tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51e5345e-c0ed-46f0-8c01-e017ffccc6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "profiler = PyTorchProfiler(profile_memory=True, schedule=torch.profiler.schedule(wait=1,warmup=1,active=2))\n",
    "logger = TensorBoardLogger(save_dir='./logs', name='GPT2-NIETZSCHE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be6797fa-a6f6-4d17-a352-f2fbd9810e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M \n",
      "------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.759   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521c569e533a4f5086c74236699954d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "FIT Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*        11.13%        4.093s        76.15%       28.019s       14.009s       3.42 Gb       1.10 Gb             2  \n",
      "                        [pl][profile]run_training_epoch         0.01%       4.492ms        34.80%       12.803s        6.402s       2.95 Gb      -8.01 Kb             2  \n",
      "                        [pl][profile]run_training_batch         0.00%     983.000us        34.78%       12.795s        6.397s       2.95 Gb          -4 b             2  \n",
      "[pl][profile][LightningModule]NietzscheTextGenerator...         0.00%      60.000us        34.77%       12.793s        6.397s       2.95 Gb           0 b             2  \n",
      "                              Optimizer.step#AdamW.step        13.05%        4.800s        34.77%       12.793s        6.397s       2.95 Gb      -2.95 Gb             2  \n",
      "[pl][profile][Callback]ModelCheckpoint{'monitor': No...        28.19%       10.370s        28.19%       10.372s        5.186s           0 b         -16 b             2  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.backward...         0.05%      19.392ms        23.57%        8.672s        4.336s      -2.03 Gb          -8 b             2  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.training...         0.10%      35.918ms        21.73%        7.993s        3.997s       5.91 Gb    -392.63 Mb             2  \n",
      "[pl][module]transformers.models.gpt2.modeling_gpt2.G...         0.00%     937.000us        21.63%        7.957s        3.978s       6.29 Gb           0 b             2  \n",
      "[pl][module]transformers.models.gpt2.modeling_gpt2.G...         0.01%       3.051ms        18.63%        6.853s        3.427s       5.53 Gb     -18.00 Mb             2  \n",
      "                                               aten::mm        14.75%        5.429s        14.75%        5.429s      27.417ms       1.80 Gb       1.80 Gb           198  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.23%      84.390ms         9.68%        3.563s      37.113ms     252.63 Mb    -900.00 Mb            96  \n",
      "                                         AddmmBackward0         0.01%       3.438ms         9.20%        3.386s      35.274ms       1.12 Gb           0 b            96  \n",
      "                                          aten::softmax         0.00%     194.000us         8.54%        3.143s     130.945ms       1.12 Gb           0 b            24  \n",
      "                                         aten::_softmax         8.54%        3.142s         8.54%        3.142s     130.937ms       1.12 Gb       1.12 Gb            24  \n",
      "                                            aten::addmm         4.23%        1.554s         4.40%        1.618s      16.849ms     648.00 Mb     648.00 Mb            96  \n",
      "                                              aten::bmm         4.07%        1.497s         4.07%        1.497s      10.396ms       2.53 Gb       2.53 Gb           144  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.07%      24.967ms         3.75%        1.380s     689.893ms     -95.16 Mb    -395.63 Mb             2  \n",
      "                                            MmBackward0         0.00%      92.000us         3.68%        1.355s     677.409ms     300.47 Mb           0 b             2  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.39%     141.969ms         3.29%        1.209s      25.192ms    -540.00 Mb      -1.86 Gb            48  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 36.792s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_module = NietzscheDataModule(file_path='nietzsche.txt', tokenizer=tokenizer, batch_size=4)\n",
    "nietzsche_generator = NietzscheTextGenerator()\n",
    "trainer = pl.Trainer(accelerator=\"auto\", max_epochs=10, logger = logger, profiler = profiler)\n",
    "trainer.fit(nietzsche_generator, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52539ec1-3ffa-41c1-b178-9ceba4e2e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100):\n",
    "    input_ids_prompt = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(input_ids_prompt)\n",
    "    nietzsche_generator.to(input_ids_prompt.device)\n",
    "    output = nietzsche_generator.model.generate(\n",
    "        input_ids_prompt,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.90,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674d939-c297-4f7a-bc69-26627c8bc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In the abyss\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
